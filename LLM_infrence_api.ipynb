{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0n1V1Dsd+myW6uSVXIQyl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikramMc/PFE/blob/main/LLM_infrence_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkEj2VleC1iW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pip3-autoremove\n",
        "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install unsloth\n",
        "# !pip install --upgrade transformers==4.52.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install flask"
      ],
      "metadata": {
        "id": "2i4txV6yDKSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "import os\n",
        "# Charger ton modèle avec un chemin vers ton modèle\n",
        "model_path =\"kimxxxx/mistral_r64_a128_g8_gas8_lr9e-5_4500tk_droplast_nopacking_2epoch\"\n",
        "#\"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"#\"unsloth/Llama-3.1-8B-unsloth-bnb-4bit\"#\"meta-llama/Llama-3.1-8B-Instruct\"#\"kimxxxx/mistral_r64_alpah128_batch8_gradient4_Ler9e-5_withoutsyntctf_ctfman_2epoch\" #\"kimxxxx/mistral_r64_alpah128_batch16_gradient2_Ler9e-5_fulldataset_2epoch\"  # Remplace par ton modèle ou ton repo HuggingFace\n",
        "token=os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "# Charger le modèle et le tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_path,\n",
        "    max_seq_length=32000,\n",
        "    dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        "    token=token,\n",
        "\n",
        ")\n",
        "\n",
        "# Préparer le modèle pour l'inférence\n",
        "FastLanguageModel.for_inference(model)\n"
      ],
      "metadata": {
        "id": "kOYkVSUUDOgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "# Initialize Flask\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate():\n",
        "    user_input = request.json.get('input', '')\n",
        "    print(\"user prompt\")\n",
        "    print(user_input)\n",
        "    print(\"**********************************\")\n",
        "    prompt = [{\"role\": \"user\", \"content\": user_input}]\n",
        "\n",
        "    formatted_prompt = tokenizer.apply_chat_template(prompt, tokenize=False) if hasattr(tokenizer, \"apply_chat_template\") else user_input\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            # Tokenize and move to device\n",
        "            inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=400,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9\n",
        "            )\n",
        "            input_length = inputs['input_ids'].shape[1]  # length of prompt tokens\n",
        "            generated_tokens = outputs[0][input_length:]  # slice off prompt tokens\n",
        "            generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        # Explicitly clear memory after generation\n",
        "        del inputs\n",
        "        del outputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        print(generated_text)\n",
        "        print(\"*********************************\")\n",
        "        return jsonify({\"response\": generated_text})\n",
        "\n",
        "    except Exception as e:\n",
        "        # Also try to clear cache in case of exceptions\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# Expose the API with ngrok\n",
        "public_url = ngrok.connect(5000)  # Ensure this is the same port your Flask app uses\n",
        "print(f\"ngrok tunnel \\\"{public_url}\\\" -> \\\"http://localhost:5000\\\"\")\n",
        "\n",
        "app.run()\n"
      ],
      "metadata": {
        "id": "EDrHsTi-DpZn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}