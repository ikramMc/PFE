{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikramMc/PFE/blob/main/llm_deployement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKouFr2c-adM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pip3-autoremove\n",
        "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install unsloth\n",
        "# !pip install --upgrade transformers==4.52.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feehvzOw-LLN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip installrequests fastapi uvicorn\n",
        "!pip install pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cqJY2k_0-cl8"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zH4OSISg-LLO",
        "outputId": "028db933-cf17-4b06-d929-9b06b08d1b3a"
      },
      "outputs": [
        {
          "ename": "TimeoutException",
          "evalue": "Requesting secret ngrok timed out. Secrets can only be fetched when running from the Colab UI.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-1104927316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mngrok_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ngrok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngrok_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutException\u001b[0m: Requesting secret ngrok timed out. Secrets can only be fetched when running from the Colab UI."
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "ngrok_key=userdata.get('ngrok')\n",
        "ngrok.set_auth_token(ngrok_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gH_cP2Hd-LLP"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import torch\n",
        "import gc\n",
        "from unsloth import FastLanguageModel\n",
        "from typing import List, Dict, Optional\n",
        "import time\n",
        "\n",
        "# === Load the LLM model with Unsloth ===\n",
        "model_path = \"kimxxxx/mistral_r32_a64_b8_gas4_lr5e-5_4500tk_2epoch\"\n",
        "token=userdata.get(HF_token)\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_path,\n",
        "    max_seq_length=32000,\n",
        "    dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        "    token=token,\n",
        ")\n",
        "\n",
        "# === Create FastAPI app ===\n",
        "app = FastAPI()\n",
        "\n",
        "# === Schemas ===\n",
        "class ChatCompletionMessage(BaseModel):\n",
        "    role: str\n",
        "    content: str\n",
        "    name: Optional[str] = None\n",
        "\n",
        "class ChatCompletionRequest(BaseModel):\n",
        "    model: str = \"mistral\"\n",
        "    messages: List[ChatCompletionMessage]\n",
        "    temperature: Optional[float] = 0.4\n",
        "    top_p: Optional[float] = 0.9\n",
        "    max_tokens: Optional[int] = 100\n",
        "\n",
        "class ChatCompletionChoice(BaseModel):\n",
        "    index: int\n",
        "    message: ChatCompletionMessage\n",
        "    finish_reason: str\n",
        "\n",
        "class ChatCompletionResponse(BaseModel):\n",
        "    id: str\n",
        "    object: str = \"chat.completion\"\n",
        "    created: int\n",
        "    model: str\n",
        "    choices: List[ChatCompletionChoice]\n",
        "    usage: Dict[str, int]\n",
        "\n",
        "# === Health Check ===\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"healthy\", \"model_loaded\": True, \"timestamp\": int(time.time())}\n",
        "\n",
        "# === Chat Completions ===\n",
        "@app.post(\"/v1/chat/completions\")\n",
        "async def create_chat_completion(request: ChatCompletionRequest):\n",
        "    try:\n",
        "        # --- Apply chat template ---\n",
        "        formatted_prompt = tokenizer.apply_chat_template(\n",
        "            [msg.dict() for msg in request.messages],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        print(\"üìù Prompt:\", formatted_prompt)\n",
        "\n",
        "        # --- Tokenize ---\n",
        "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "        input_length = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "        # --- Generate text ---\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=request.max_tokens,\n",
        "                temperature=request.temperature,\n",
        "                top_p=request.top_p,\n",
        "                do_sample=True,\n",
        "            )\n",
        "\n",
        "        # --- Keep only the generated tokens ---\n",
        "        generated_tokens = outputs[0][input_length:]\n",
        "\n",
        "        # --- Decode result ---\n",
        "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "        print(\"ü§ñ Response:\", generated_text)\n",
        "\n",
        "        # Cleanup\n",
        "        del inputs, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # --- Build API response ---\n",
        "        response_message = ChatCompletionMessage(role=\"assistant\", content=generated_text)\n",
        "\n",
        "        response = ChatCompletionResponse(\n",
        "            id=f\"chatcmpl-{int(time.time())}\",\n",
        "            created=int(time.time()),\n",
        "            model=request.model,\n",
        "            choices=[ChatCompletionChoice(index=0, message=response_message, finish_reason=\"stop\")],\n",
        "            usage={\n",
        "                \"prompt_tokens\": input_length,\n",
        "                \"completion_tokens\": len(generated_tokens),\n",
        "                \"total_tokens\": input_length + len(generated_tokens),\n",
        "            },\n",
        "        )\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"‚ùå Error:\", str(e))\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "weAtVTOM-LLQ"
      },
      "outputs": [],
      "source": [
        "# Add this at the top of your imports\n",
        "import atexit\n",
        "from threading import Event\n",
        "\n",
        "# Global variables for cleanup\n",
        "server_thread = None\n",
        "stop_event = Event()\n",
        "ngrok_tunnel = None\n",
        "\n",
        "# === ngrok setup ===\n",
        "if __name__ == \"__main__\":\n",
        "    import nest_asyncio\n",
        "    from pyngrok import ngrok\n",
        "    from threading import Thread\n",
        "    import asyncio\n",
        "    from uvicorn import Config, Server\n",
        "\n",
        "    # Cleanup any previous instances\n",
        "    stop_event.set()\n",
        "    if server_thread and server_thread.is_alive():\n",
        "        server_thread.join()\n",
        "    if ngrok_tunnel:\n",
        "        ngrok.disconnect(ngrok_tunnel.public_url)\n",
        "\n",
        "    # Reset for new run\n",
        "    stop_event.clear()\n",
        "\n",
        "    # 1. Public tunnel with ngrok\n",
        "    ngrok_tunnel = ngrok.connect(8001)\n",
        "    print(\"‚úÖ Public URL:\", ngrok_tunnel.public_url)\n",
        "\n",
        "    # 2. Patch asyncio for Jupyter environments\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    # 3. Run server in a thread with its own event loop\n",
        "    def run_api():\n",
        "        config = Config(app=app, host=\"0.0.0.0\", port=8001, log_level=\"info\")\n",
        "        server = Server(config=config)\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "\n",
        "        # Run until stop event is set\n",
        "        while not stop_event.is_set():\n",
        "            loop.run_until_complete(server.serve())\n",
        "        loop.close()\n",
        "\n",
        "    server_thread = Thread(target=run_api)\n",
        "    server_thread.start()\n",
        "\n",
        "    # Register cleanup at exit\n",
        "    def cleanup():\n",
        "        stop_event.set()\n",
        "        if ngrok_tunnel:\n",
        "            ngrok.disconnect(ngrok_tunnel.public_url)\n",
        "        if server_thread and server_thread.is_alive():\n",
        "            server_thread.join()\n",
        "\n",
        "    atexit.register(cleanup)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}