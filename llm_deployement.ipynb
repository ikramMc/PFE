{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikramMc/PFE/blob/main/llm_deployement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1"
      ],
      "metadata": {
        "id": "dKouFr2c-adM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-cors\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "feehvzOw-LLN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T09:25:20.425772Z",
          "iopub.execute_input": "2025-07-15T09:25:20.426062Z",
          "iopub.status.idle": "2025-07-15T09:25:23.830675Z",
          "shell.execute_reply.started": "2025-07-15T09:25:20.426023Z",
          "shell.execute_reply": "2025-07-15T09:25:23.829974Z"
        },
        "id": "7O4dhSm3-LLO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install gradio requests fastapi uvicorn"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T09:25:05.951623Z",
          "iopub.execute_input": "2025-07-15T09:25:05.951910Z",
          "iopub.status.idle": "2025-07-15T09:25:20.424925Z",
          "shell.execute_reply.started": "2025-07-15T09:25:05.951888Z",
          "shell.execute_reply": "2025-07-15T09:25:20.424210Z"
        },
        "collapsed": true,
        "id": "YY61DG-L-LLO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ],
      "metadata": {
        "id": "cqJY2k_0-cl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "ngrok_key=userdata.get('ngrok')\n",
        "ngrok.set_auth_token(ngrok_key)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T09:25:23.832471Z",
          "iopub.execute_input": "2025-07-15T09:25:23.832728Z",
          "iopub.status.idle": "2025-07-15T09:25:24.929632Z",
          "shell.execute_reply.started": "2025-07-15T09:25:23.832706Z",
          "shell.execute_reply": "2025-07-15T09:25:24.929053Z"
        },
        "id": "zH4OSISg-LLO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Body, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import torch\n",
        "import gc\n",
        "from unsloth import FastLanguageModel\n",
        "from typing import List, Dict, Optional\n",
        "import time\n",
        "\n",
        "# Load the LLM model with Unsloth\n",
        "model_path = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
        "token=userdata.get(HF_token)\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_path,\n",
        "    max_seq_length=32000,\n",
        "    dtype=torch.float16,\n",
        "    load_in_4bit=True,\n",
        "    token=token,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# === Create FastAPI app ===\n",
        "app = FastAPI()\n",
        "SESSIONS = {}\n",
        "\n",
        "# OpenAI compatible schemas\n",
        "class ChatCompletionMessage(BaseModel):\n",
        "    role: str  # \"user\", \"assistant\", or \"system\"\n",
        "    content: str\n",
        "    name: Optional[str] = None\n",
        "\n",
        "class ChatCompletionRequest(BaseModel):\n",
        "    model: str = \"mistral\"\n",
        "    messages: List[ChatCompletionMessage]\n",
        "    temperature: Optional[float] = 0.7\n",
        "    top_p: Optional[float] = 0.9\n",
        "    max_tokens: Optional[int] = 400\n",
        "\n",
        "class ChatCompletionChoice(BaseModel):\n",
        "    index: int\n",
        "    message: ChatCompletionMessage\n",
        "    finish_reason: str\n",
        "\n",
        "class ChatCompletionResponse(BaseModel):\n",
        "    id: str\n",
        "    object: str = \"chat.completion\"\n",
        "    created: int\n",
        "    model: str\n",
        "    choices: List[ChatCompletionChoice]\n",
        "    usage: Dict[str, int]\n",
        "\n",
        "# === Health Check Endpoint ===\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"model_loaded\": True,\n",
        "        \"timestamp\": int(time.time())\n",
        "    }\n",
        "\n",
        "# === OpenAI compatible endpoints ===\n",
        "@app.post(\"/v1/chat/completions\")\n",
        "async def create_chat_completion(request: ChatCompletionRequest):\n",
        "    try:\n",
        "        # Convert messages to chat template format\n",
        "        chat_history = [{\"role\": msg.role, \"content\": msg.content} for msg in request.messages]\n",
        "\n",
        "        formatted_prompt = tokenizer.apply_chat_template(chat_history, tokenize=False) if hasattr(tokenizer, \"apply_chat_template\") else request.messages[-1].content\n",
        "\n",
        "        print(\"\\n📝 PROMPT SENT TO MODEL:\\n\")\n",
        "        print(formatted_prompt)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=request.max_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=request.temperature,\n",
        "                top_p=request.top_p\n",
        "            )\n",
        "            input_length = inputs['input_ids'].shape[1]\n",
        "            generated_tokens = outputs[0][input_length:]\n",
        "            generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        print(\"\\n🤖 MODEL GENERATED RESPONSE:\\n\")\n",
        "        print(generated_text)\n",
        "\n",
        "        # Clean up\n",
        "        del inputs, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Prepare OpenAI compatible response\n",
        "        response_message = ChatCompletionMessage(\n",
        "            role=\"assistant\",\n",
        "            content=generated_text\n",
        "        )\n",
        "\n",
        "        response = ChatCompletionResponse(\n",
        "            id=f\"chatcmpl-{int(time.time())}\",\n",
        "            created=int(time.time()),\n",
        "            model=request.model,\n",
        "            choices=[ChatCompletionChoice(\n",
        "                index=0,\n",
        "                message=response_message,\n",
        "                finish_reason=\"stop\"\n",
        "            )],\n",
        "            usage={\n",
        "                \"prompt_tokens\": input_length,\n",
        "                \"completion_tokens\": len(generated_tokens),\n",
        "                \"total_tokens\": input_length + len(generated_tokens)\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"❌ Error:\", str(e))\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T09:25:24.930344Z",
          "iopub.execute_input": "2025-07-15T09:25:24.930597Z",
          "iopub.status.idle": "2025-07-15T09:26:41.725219Z",
          "shell.execute_reply.started": "2025-07-15T09:25:24.930579Z",
          "shell.execute_reply": "2025-07-15T09:26:41.724608Z"
        },
        "id": "gH_cP2Hd-LLP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this at the top of your imports\n",
        "import atexit\n",
        "from threading import Event\n",
        "\n",
        "# Global variables for cleanup\n",
        "server_thread = None\n",
        "stop_event = Event()\n",
        "ngrok_tunnel = None\n",
        "\n",
        "# === ngrok setup ===\n",
        "if __name__ == \"__main__\":\n",
        "    import nest_asyncio\n",
        "    from pyngrok import ngrok\n",
        "    from threading import Thread\n",
        "    import asyncio\n",
        "    from uvicorn import Config, Server\n",
        "\n",
        "    # Cleanup any previous instances\n",
        "    stop_event.set()\n",
        "    if server_thread and server_thread.is_alive():\n",
        "        server_thread.join()\n",
        "    if ngrok_tunnel:\n",
        "        ngrok.disconnect(ngrok_tunnel.public_url)\n",
        "\n",
        "    # Reset for new run\n",
        "    stop_event.clear()\n",
        "\n",
        "    # 1. Public tunnel with ngrok\n",
        "    ngrok_tunnel = ngrok.connect(8001)\n",
        "    print(\"✅ Public URL:\", ngrok_tunnel.public_url)\n",
        "\n",
        "    # 2. Patch asyncio for Jupyter environments\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    # 3. Run server in a thread with its own event loop\n",
        "    def run_api():\n",
        "        config = Config(app=app, host=\"0.0.0.0\", port=8001, log_level=\"info\")\n",
        "        server = Server(config=config)\n",
        "        loop = asyncio.new_event_loop()\n",
        "        asyncio.set_event_loop(loop)\n",
        "\n",
        "        # Run until stop event is set\n",
        "        while not stop_event.is_set():\n",
        "            loop.run_until_complete(server.serve())\n",
        "        loop.close()\n",
        "\n",
        "    server_thread = Thread(target=run_api)\n",
        "    server_thread.start()\n",
        "\n",
        "    # Register cleanup at exit\n",
        "    def cleanup():\n",
        "        stop_event.set()\n",
        "        if ngrok_tunnel:\n",
        "            ngrok.disconnect(ngrok_tunnel.public_url)\n",
        "        if server_thread and server_thread.is_alive():\n",
        "            server_thread.join()\n",
        "\n",
        "    atexit.register(cleanup)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-15T09:26:41.725947Z",
          "iopub.execute_input": "2025-07-15T09:26:41.726207Z",
          "iopub.status.idle": "2025-07-15T09:26:42.007084Z",
          "shell.execute_reply.started": "2025-07-15T09:26:41.726183Z",
          "shell.execute_reply": "2025-07-15T09:26:42.006230Z"
        },
        "id": "weAtVTOM-LLQ"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}