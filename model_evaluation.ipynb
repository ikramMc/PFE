{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBDLAZEb+483NXMYVeBB3d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikramMc/PFE/blob/main/model_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5yab__SLa9X"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install  sentence-transformers  nltk bert-score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pip3-autoremove\n",
        "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install unsloth\n",
        "# !pip install --upgrade transformers==4.52.3"
      ],
      "metadata": {
        "id": "I71I8uooLojB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation on test split using NLG and semantic metrics**"
      ],
      "metadata": {
        "id": "HRvr5YSnL5Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_token=userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "e926UfXSd8jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from unsloth import FastLanguageModel\n",
        "import random\n",
        "import torch\n",
        "import gc\n",
        "import ast\n",
        "import re\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "from bert_score import score as bert_score\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from rouge_score import rouge_scorer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"test_with_sub_conversations.csv\")\n",
        "\n",
        "def parse_conversation(conv_str):\n",
        "    fixed_str = re.sub(r'\\}\\s*\\{', '}, {', conv_str)\n",
        "    try:\n",
        "        return ast.literal_eval(fixed_str)\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "data[\"parsed_conversations\"] = data[\"conversations\"].apply(parse_conversation)\n",
        "\n",
        "multi_turn_convs = data[\"parsed_conversations\"].apply(lambda x: len(x) >= 4)\n",
        "single_turn_convs = data[\"parsed_conversations\"].apply(lambda x: len(x) == 2)\n",
        "\n",
        "multi_turn_data = data[multi_turn_convs][\"parsed_conversations\"].tolist()\n",
        "single_turn_data = data[single_turn_convs][\"parsed_conversations\"].tolist()\n",
        "\n",
        "multi_turn_sample = random.sample(multi_turn_data, min(1000, len(multi_turn_data)))\n",
        "single_turn_sample = random.sample(single_turn_data, min(1000, len(single_turn_data)))\n",
        "\n",
        "print(f\"Multi-turn conversations: {len(multi_turn_data)}\")\n",
        "print(f\"Single-turn conversations: {len(single_turn_data)}\")\n",
        "\n",
        "models = {\n",
        "    \"qwen base\": \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\",\n",
        "    # Add other models as needed\n",
        "}\n",
        "\n",
        "# Initialize evaluation tools\n",
        "embedder = SentenceTransformer(\"intfloat/e5-large-v2\")\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "results = []\n",
        "\n",
        "def calculate_meteor(predictions, references):\n",
        "    \"\"\"Calculate METEOR scores for a list of predictions and references\"\"\"\n",
        "    meteor_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        try:\n",
        "            # Tokenize the sentences\n",
        "            pred_tokens = nltk.word_tokenize(pred.lower())\n",
        "            ref_tokens = nltk.word_tokenize(ref.lower())\n",
        "\n",
        "            # Calculate METEOR score\n",
        "            meteor_score = single_meteor_score(ref_tokens, pred_tokens)\n",
        "            meteor_scores.append(meteor_score)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating METEOR: {e}\")\n",
        "            meteor_scores.append(0.0)\n",
        "\n",
        "    return sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0.0\n",
        "\n",
        "def evaluate(model_name, model_path, conversations, mode=\"multi\"):\n",
        "    print(f\"üîç Evaluating {model_name} on {mode}-turn\")\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_path,\n",
        "        max_seq_length=5000,\n",
        "        dtype=torch.float16,\n",
        "        load_in_4bit=True,\n",
        "        token=HF_token\n",
        "    )\n",
        "\n",
        "    # Apply chat template\n",
        "    tokenizer = get_chat_template(\n",
        "        tokenizer,\n",
        "        chat_template=\"qwen-2.5\",\n",
        "    )\n",
        "\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    predictions, references = [], []\n",
        "\n",
        "    for convo in conversations:\n",
        "        for i in range(0, len(convo), 2):\n",
        "            if i + 1 >= len(convo) or convo[i][\"role\"] != \"user\" or convo[i + 1][\"role\"] != \"assistant\":\n",
        "                continue\n",
        "\n",
        "            # Prepare conversation history up to current user message\n",
        "            history = convo[:i + 1]\n",
        "            prompt = tokenizer.apply_chat_template(history, tokenize=False, add_generation_prompt=True)\n",
        "            reference = convo[i + 1][\"content\"]\n",
        "\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    # Tokenize input\n",
        "                    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "                    input_length = inputs['input_ids'].shape[1]\n",
        "\n",
        "                    print(f\"Prompt: {prompt[:200]}...\")  # Show first 200 chars\n",
        "                    print(\"=\" * 50)\n",
        "\n",
        "                    # Generate response\n",
        "                    outputs = model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=200,\n",
        "                        do_sample=True,\n",
        "                        temperature=0.7,\n",
        "                        top_p=0.9,\n",
        "                        pad_token_id=tokenizer.eos_token_id\n",
        "                    )\n",
        "\n",
        "                    # Extract only the generated part (new tokens)\n",
        "                    decoded_response = tokenizer.decode(outputs[0, input_length:], skip_special_tokens=True)\n",
        "                    prediction = decoded_response.strip()\n",
        "\n",
        "                    print(f\"Generated: {prediction}\")\n",
        "                    print(f\"Reference: {reference}\")\n",
        "                    print(\"-\" * 50)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during generation: {e}\")\n",
        "                prediction = \"\"\n",
        "\n",
        "            predictions.append(prediction)\n",
        "            references.append(reference)\n",
        "\n",
        "            # Clean up GPU memory\n",
        "            del inputs, outputs\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    rouge_results = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        scores = scorer.score(ref, pred)\n",
        "        for key in rouge_results:\n",
        "            rouge_results[key].append(scores[key])\n",
        "\n",
        "    # Average ROUGE scores (precision, recall, f-measure)\n",
        "    avg_rouge_scores = {}\n",
        "    for key in rouge_results:\n",
        "        avg_rouge_scores[f\"{key}_precision\"] = sum([s.precision for s in rouge_results[key]]) / len(rouge_results[key])\n",
        "        avg_rouge_scores[f\"{key}_recall\"] = sum([s.recall for s in rouge_results[key]]) / len(rouge_results[key])\n",
        "        avg_rouge_scores[f\"{key}_fmeasure\"] = sum([s.fmeasure for s in rouge_results[key]]) / len(rouge_results[key])\n",
        "\n",
        "    # Calculate embedding similarity\n",
        "    embeddings_preds = embedder.encode(predictions, convert_to_tensor=True)\n",
        "    embeddings_refs = embedder.encode(references, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(embeddings_preds, embeddings_refs).diagonal()\n",
        "    avg_similarity = similarities.mean().item()\n",
        "\n",
        "    # Calculate BLEU scores\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    bleu_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        try:\n",
        "            bleu_score = sentence_bleu([nltk.word_tokenize(ref)], nltk.word_tokenize(pred), smoothing_function=smoothing)\n",
        "            bleu_scores.append(bleu_score)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating BLEU: {e}\")\n",
        "            bleu_scores.append(0.0)\n",
        "\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
        "\n",
        "    # Calculate BERTScore (Precision, Recall, F1)\n",
        "    try:\n",
        "        P, R, F1 = bert_score(predictions, references, lang=\"en\", verbose=False)\n",
        "        avg_bertscore_precision = P.mean().item()\n",
        "        avg_bertscore_recall = R.mean().item()\n",
        "        avg_bertscore_f1 = F1.mean().item()\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating BERTScore: {e}\")\n",
        "        avg_bertscore_precision = avg_bertscore_recall = avg_bertscore_f1 = 0.0\n",
        "\n",
        "    # Calculate METEOR score\n",
        "    avg_meteor = calculate_meteor(predictions, references)\n",
        "\n",
        "    # Parse model metadata (if available)\n",
        "    metadata = re.findall(r'r(\\d+)_alpah(\\d+)_batch(\\d+)_gradient(\\d+)_Ler([\\de\\.-]+)(_cste)?_?fulldataset(?:_ctfman)?_(\\d+)?', model_path)\n",
        "    if metadata:\n",
        "        rank, alpha, batch, gradient, lr, constant, epoch = metadata[0]\n",
        "        scheduler = \"constant\" if constant else \"cosine\"\n",
        "        dataset = \"ctfman\" if \"ctfman\" in model_path else \"fulldataset\"\n",
        "    else:\n",
        "        rank = alpha = batch = gradient = lr = epoch = \"\"\n",
        "        scheduler = dataset = \"\"\n",
        "\n",
        "    # Store results\n",
        "    result_entry = {\n",
        "        \"model_name\": model_name,\n",
        "        \"mode\": mode,\n",
        "        \"rank\": rank,\n",
        "        \"alpha\": alpha,\n",
        "        \"gradient\": gradient,\n",
        "        \"batch\": batch,\n",
        "        \"learning_rate\": lr,\n",
        "        \"scheduler\": scheduler,\n",
        "        \"dataset\": dataset,\n",
        "        \"epoch\": epoch,\n",
        "        **avg_rouge_scores,\n",
        "        \"embedding_similarity\": avg_similarity,\n",
        "        \"bleu\": avg_bleu,\n",
        "        \"bertscore_precision\": avg_bertscore_precision,\n",
        "        \"bertscore_recall\": avg_bertscore_recall,\n",
        "        \"bertscore_f1\": avg_bertscore_f1,\n",
        "        \"meteor\": avg_meteor,\n",
        "        \"sample_prediction\": predictions[0] if predictions else \"\",\n",
        "        \"sample_reference\": references[0] if references else \"\",\n",
        "        \"total_samples\": len(predictions)\n",
        "    }\n",
        "\n",
        "    results.append(result_entry)\n",
        "\n",
        "    # Clean up model from memory\n",
        "    del model, tokenizer\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Run evaluation\n",
        "for name, path in models.items():\n",
        "    evaluate(name, path, multi_turn_sample, mode=\"multi\")\n",
        "    evaluate(name, path, single_turn_sample, mode=\"single\")\n",
        "\n",
        "# Save results\n",
        "final_df = pd.DataFrame(results)\n",
        "final_df.to_csv(\"model_evaluation_results.csv\", index=False)\n",
        "\n",
        "print(\"Evaluation completed!\")\n",
        "print(f\"Results saved to model_evaluation_results.csv\")\n",
        "print(\"\\nSample results:\")\n",
        "print(final_df[['model_name', 'mode', 'rouge1_fmeasure', 'bleu', 'bertscore_f1', 'meteor', 'embedding_similarity']].head())\n",
        "final_df = pd.DataFrame(results)\n",
        "final_df.to_csv(\"model_evaluation_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "PxjTFvkwLsH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MMLU Evaluation**"
      ],
      "metadata": {
        "id": "jtvWzozJMUN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import gc\n",
        "import random\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from tqdm import tqdm  # ‚Üê Import tqdm for progress bar\n",
        "\n",
        "# === Load all MMLU subcategories ===\n",
        "mmlu_dataset = load_dataset(\"cais/mmlu\", \"all\", trust_remote_code=True)\n",
        "\n",
        "# Concatenate all test sets\n",
        "all_subcategories = mmlu_dataset['test']\n",
        "full_test_dataset = concatenate_datasets([\n",
        "    all_subcategories.filter(lambda x: x[\"choices\"] is not None)\n",
        "])\n",
        "\n",
        "# Sample 1000 examples randomly\n",
        "sampled_dataset = full_test_dataset.shuffle(seed=42).select(range(1000))\n",
        "\n",
        "# === Model setup ===\n",
        "models = {\n",
        "    \"mistral base\": \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"mistral fine-tuned\": \"kimxxxx/mistral_r64_a128_g8_gas8_lr9e-5_4500tk_droplast_3epoch\"\n",
        "}\n",
        "\n",
        "# === Format MMLU prompt ===\n",
        "def format_mmlu_prompt(example):\n",
        "    return (\n",
        "        f\"Question: {example['question']}\\n\"\n",
        "        f\"A. {example['choices'][0]}\\n\"\n",
        "        f\"B. {example['choices'][1]}\\n\"\n",
        "        f\"C. {example['choices'][2]}\\n\"\n",
        "        f\"D. {example['choices'][3]}\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "# === Evaluate function ===\n",
        "def evaluate_on_mmlu(model_name, model_path, dataset):\n",
        "    print(f\"üß™ Evaluating {model_name} on MMLU (1000 samples)...\")\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_path,\n",
        "        max_seq_length=2048,\n",
        "        dtype=torch.float16,\n",
        "        load_in_4bit=True,\n",
        "        token=HF_token\n",
        "    )\n",
        "    tokenizer = get_chat_template(tokenizer, chat_template=\"qwen-2.5\")\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    correct = 0\n",
        "    results = []\n",
        "\n",
        "    for idx, example in enumerate(tqdm(dataset, desc=f\"Evaluating {model_name}\", ncols=100)):\n",
        "        prompt = format_mmlu_prompt(example)\n",
        "        correct_answer = example[\"answer\"]\n",
        "\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "                outputs = model.generate(**inputs, max_new_tokens=5)\n",
        "                prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                prediction = prediction.strip().upper()\n",
        "\n",
        "            letter = next((c for c in prediction if c in \"ABCD\"), \"\")\n",
        "\n",
        "            is_correct = (letter == [\"A\", \"B\", \"C\", \"D\"][correct_answer])\n",
        "            correct += int(is_correct)\n",
        "\n",
        "            results.append({\n",
        "                \"index\": idx,\n",
        "                \"subject\": example[\"subject\"],\n",
        "                \"question\": example[\"question\"],\n",
        "                \"prediction\": letter,\n",
        "                \"correct_answer\": [\"A\", \"B\", \"C\", \"D\"][correct_answer],\n",
        "                \"is_correct\": is_correct,\n",
        "                \"raw_output\": prediction\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error on item {idx}: {e}\")\n",
        "            results.append({\n",
        "                \"index\": idx,\n",
        "                \"subject\": example[\"subject\"],\n",
        "                \"question\": example[\"question\"],\n",
        "                \"prediction\": \"ERROR\",\n",
        "                \"correct_answer\": [\"A\", \"B\", \"C\", \"D\"][correct_answer],\n",
        "                \"is_correct\": False,\n",
        "                \"raw_output\": str(e)\n",
        "            })\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    accuracy = correct / len(dataset) * 100\n",
        "    print(f\"‚úÖ {model_name} Accuracy on MMLU (1000 samples): {accuracy:.2f}%\")\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# === Run evaluation ===\n",
        "for name, path in models.items():\n",
        "    df = evaluate_on_mmlu(name, path, sampled_dataset)\n",
        "    df.to_csv(f\"{name.replace(' ', '_').lower()}_mmlu_1000sample_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "X7_OdB68dg1X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}