{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikramMc/PFE/blob/main/model_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5yab__SLa9X"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install  sentence-transformers  nltk bert-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I71I8uooLojB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pip3-autoremove\n",
        "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install unsloth\n",
        "# !pip install --upgrade transformers==4.52.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e926UfXSd8jt"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "HF_token=userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRvr5YSnL5Vd"
      },
      "source": [
        "# **Evaluation on test split using NLG and semantic similarity metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "f37qw3hIONPd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from unsloth import FastLanguageModel\n",
        "import random\n",
        "import torch\n",
        "import gc\n",
        "import ast\n",
        "import re\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import single_meteor_score\n",
        "from bert_score import score as bert_score\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from rouge_score import rouge_scorer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"test+comptia.csv\")#(\"test_with_sub_conversations.csv\")\n",
        "\n",
        "def parse_conversation(conv_str):\n",
        "    fixed_str = re.sub(r'\\}\\s*\\{', '}, {', conv_str)\n",
        "    try:\n",
        "        return ast.literal_eval(fixed_str)\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "data[\"parsed_conversations\"] = data[\"conversations\"].apply(parse_conversation)\n",
        "\n",
        "multi_turn_convs = data[\"parsed_conversations\"].apply(lambda x: len(x) >= 4)\n",
        "single_turn_convs = data[\"parsed_conversations\"].apply(lambda x: len(x) == 2)\n",
        "\n",
        "multi_turn_data = data[multi_turn_convs][\"parsed_conversations\"].tolist()\n",
        "single_turn_data = data[single_turn_convs][\"parsed_conversations\"].tolist()\n",
        "\n",
        "multi_turn_sample = random.sample(multi_turn_data, min(1000, len(multi_turn_data)))\n",
        "single_turn_sample = random.sample(single_turn_data, min(1000, len(single_turn_data)))\n",
        "\n",
        "print(f\"Multi-turn conversations: {len(multi_turn_data)}\")\n",
        "print(f\"Single-turn conversations: {len(single_turn_data)}\")\n",
        "\n",
        "models = {\n",
        "    #\"qwen base\": \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\",\n",
        "    #\"mistral base\": \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    #\"mistral fine-tuned\": \"kimxxxx/mistral_r16_a32_b8_gas2_lr5e-5_4500tk_2epoch_test\"\n",
        "    #\"phi base\":\"unsloth/Phi-3.5-mini-instruct-bnb-4bit\"\n",
        "    \"llama3.1 finetuned 3 epochs\":\"kimxxxx/llama_r32_a64_b16_gas2_lr5e-5_4500tk_3epoch\"\n",
        "    # Add other models as needed\n",
        "}\n",
        "\n",
        "# Initialize evaluation tools (only once)\n",
        "embedder = SentenceTransformer(\"intfloat/e5-large-v2\")\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "results = []\n",
        "\n",
        "def cleanup_gpu_memory():\n",
        "    \"\"\"Force cleanup of GPU memory\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def get_chat_template_type(model_path):\n",
        "    \"\"\"Determine the appropriate chat template based on model path\"\"\"\n",
        "    model_path_lower = model_path.lower()\n",
        "\n",
        "    if \"qwen\" in model_path_lower:\n",
        "        return \"qwen\"\n",
        "    elif \"mistral\" in model_path_lower:\n",
        "        return \"mistral\"\n",
        "    elif \"llama\" in model_path_lower:\n",
        "        return \"llama\"\n",
        "    elif \"gemma\" in model_path_lower:\n",
        "        return \"gemma\"\n",
        "    elif \"phi\" in model_path_lower:\n",
        "        return \"phi\"\n",
        "    else:\n",
        "        # Default fallback - try to use the tokenizer's native template\n",
        "        return None\n",
        "\n",
        "def calculate_meteor(predictions, references):\n",
        "    \"\"\"Calculate METEOR scores for a list of predictions and references\"\"\"\n",
        "    meteor_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        try:\n",
        "            # Tokenize the sentences\n",
        "            pred_tokens = nltk.word_tokenize(pred.lower())\n",
        "            ref_tokens = nltk.word_tokenize(ref.lower())\n",
        "\n",
        "            # Calculate METEOR score\n",
        "            meteor_score = single_meteor_score(ref_tokens, pred_tokens)\n",
        "            meteor_scores.append(meteor_score)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating METEOR: {e}\")\n",
        "            meteor_scores.append(0.0)\n",
        "\n",
        "    return sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0.0\n",
        "\n",
        "def evaluate_conversations(model, tokenizer, conversations, model_name, mode):\n",
        "    \"\"\"Evaluate model on a set of conversations\"\"\"\n",
        "    predictions, references = [], []\n",
        "\n",
        "    for convo in conversations:\n",
        "        for i in range(0, len(convo), 2):\n",
        "            if i + 1 >= len(convo) or convo[i][\"role\"] != \"user\" or convo[i + 1][\"role\"] != \"assistant\":\n",
        "                continue\n",
        "\n",
        "            # Prepare conversation history up to current user message\n",
        "            history = convo[:i + 1]\n",
        "\n",
        "            # Apply chat template\n",
        "            try:\n",
        "                prompt = tokenizer.apply_chat_template(history, tokenize=False, add_generation_prompt=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error applying chat template: {e}\")\n",
        "                # Fallback to simple concatenation if chat template fails\n",
        "                prompt = \"\"\n",
        "                for msg in history:\n",
        "                    prompt += f\"{msg['role']}: {msg['content']}\\n\"\n",
        "                prompt += \"assistant: \"\n",
        "\n",
        "            reference = convo[i + 1][\"content\"]\n",
        "\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    # Tokenize input\n",
        "                    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "                    input_length = inputs['input_ids'].shape[1]\n",
        "\n",
        "                    #print(f\"Prompt: {prompt[:200]}...\")  # Show first 200 chars\n",
        "                    #print(\"=\" * 50)\n",
        "\n",
        "                    # Generate response\n",
        "                    outputs = model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=200,\n",
        "                        do_sample=True,\n",
        "                        temperature=0.7,\n",
        "                        top_p=0.9,\n",
        "                        pad_token_id=tokenizer.eos_token_id\n",
        "                    )\n",
        "\n",
        "                    # Extract only the generated part (new tokens)\n",
        "                    decoded_response = tokenizer.decode(outputs[0, input_length:], skip_special_tokens=True)\n",
        "                    prediction = decoded_response.strip()\n",
        "\n",
        "                    #print(f\"Generated: {prediction}\")\n",
        "                    #print(f\"Reference: {reference}\")\n",
        "                    #print(\"-\" * 50)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during generation: {e}\")\n",
        "                prediction = \"\"\n",
        "\n",
        "            predictions.append(prediction)\n",
        "            references.append(reference)\n",
        "\n",
        "            # Clean up intermediate tensors\n",
        "            if 'inputs' in locals():\n",
        "                del inputs\n",
        "            if 'outputs' in locals():\n",
        "                del outputs\n",
        "            cleanup_gpu_memory()\n",
        "\n",
        "    return predictions, references\n",
        "\n",
        "def calculate_metrics(predictions, references):\n",
        "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
        "    metrics = {}\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    rouge_results = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        scores = scorer.score(ref, pred)\n",
        "        for key in rouge_results:\n",
        "            rouge_results[key].append(scores[key])\n",
        "\n",
        "    # Average ROUGE scores (precision, recall, f-measure)\n",
        "    for key in rouge_results:\n",
        "        metrics[f\"{key}_precision\"] = sum([s.precision for s in rouge_results[key]]) / len(rouge_results[key])\n",
        "        metrics[f\"{key}_recall\"] = sum([s.recall for s in rouge_results[key]]) / len(rouge_results[key])\n",
        "        metrics[f\"{key}_fmeasure\"] = sum([s.fmeasure for s in rouge_results[key]]) / len(rouge_results[key])\n",
        "\n",
        "    # Calculate embedding similarity\n",
        "    embeddings_preds = embedder.encode(predictions, convert_to_tensor=True)\n",
        "    embeddings_refs = embedder.encode(references, convert_to_tensor=True)\n",
        "    similarities = util.cos_sim(embeddings_preds, embeddings_refs).diagonal()\n",
        "    metrics[\"embedding_similarity\"] = similarities.mean().item()\n",
        "\n",
        "    # Calculate BLEU scores\n",
        "    smoothing = SmoothingFunction().method1\n",
        "    bleu_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        try:\n",
        "            bleu_score = sentence_bleu([nltk.word_tokenize(ref)], nltk.word_tokenize(pred), smoothing_function=smoothing)\n",
        "            bleu_scores.append(bleu_score)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating BLEU: {e}\")\n",
        "            bleu_scores.append(0.0)\n",
        "\n",
        "    metrics[\"bleu\"] = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
        "\n",
        "    # Calculate BERTScore (Precision, Recall, F1)\n",
        "    try:\n",
        "        P, R, F1 = bert_score(predictions, references, lang=\"en\", verbose=False)\n",
        "        metrics[\"bertscore_precision\"] = P.mean().item()\n",
        "        metrics[\"bertscore_recall\"] = R.mean().item()\n",
        "        metrics[\"bertscore_f1\"] = F1.mean().item()\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating BERTScore: {e}\")\n",
        "        metrics[\"bertscore_precision\"] = metrics[\"bertscore_recall\"] = metrics[\"bertscore_f1\"] = 0.0\n",
        "\n",
        "    # Calculate METEOR score\n",
        "    metrics[\"meteor\"] = calculate_meteor(predictions, references)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def evaluate_model_complete(model_name, model_path):\n",
        "    \"\"\"Evaluate a model on both multi-turn and single-turn conversations\"\"\"\n",
        "    print(f\"ðŸ” Loading model: {model_name}\")\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=model_path,\n",
        "        max_seq_length=5000,\n",
        "        dtype=torch.float16,\n",
        "        load_in_4bit=True,\n",
        "        token=HF_token  # Make sure to define this variable\n",
        "    )\n",
        "\n",
        "    # Determine and apply appropriate chat template\n",
        "    template_type = get_chat_template_type(model_path)\n",
        "\n",
        "    if template_type:\n",
        "        print(f\"Using chat template: {template_type}\")\n",
        "        tokenizer = get_chat_template(tokenizer, chat_template=template_type)\n",
        "    else:\n",
        "        print(\"Using tokenizer's default chat template\")\n",
        "        # Check if tokenizer already has a chat template\n",
        "        if not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:\n",
        "            print(\"Warning: No chat template found, using fallback\")\n",
        "            # You might want to set a default template here if needed\n",
        "\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    # Evaluate on multi-turn conversations\n",
        "    print(f\"ðŸ” Evaluating {model_name} on multi-turn\")\n",
        "    multi_predictions, multi_references = evaluate_conversations(\n",
        "        model, tokenizer, multi_turn_sample, model_name, \"multi\"\n",
        "    )\n",
        "\n",
        "    # Calculate metrics for multi-turn\n",
        "    multi_metrics = calculate_metrics(multi_predictions, multi_references)\n",
        "\n",
        "    # Parse model metadata (if available)\n",
        "    metadata = re.findall(r'r(\\d+)_alpah(\\d+)_batch(\\d+)_gradient(\\d+)_Ler([\\de\\.-]+)(_cste)?_?fulldataset(?:_ctfman)?_(\\d+)?', model_path)\n",
        "    if metadata:\n",
        "        rank, alpha, batch, gradient, lr, constant, epoch = metadata[0]\n",
        "        scheduler = \"constant\" if constant else \"cosine\"\n",
        "        dataset = \"ctfman\" if \"ctfman\" in model_path else \"fulldataset\"\n",
        "    else:\n",
        "        rank = alpha = batch = gradient = lr = epoch = \"\"\n",
        "        scheduler = dataset = \"\"\n",
        "\n",
        "    # Store multi-turn results\n",
        "    multi_result = {\n",
        "        \"model_name\": model_name,\n",
        "        \"mode\": \"multi\",\n",
        "        \"rank\": rank,\n",
        "        \"alpha\": alpha,\n",
        "        \"gradient\": gradient,\n",
        "        \"batch\": batch,\n",
        "        \"learning_rate\": lr,\n",
        "        \"scheduler\": scheduler,\n",
        "        \"dataset\": dataset,\n",
        "        \"epoch\": epoch,\n",
        "        **multi_metrics,\n",
        "        \"sample_prediction\": multi_predictions[0] if multi_predictions else \"\",\n",
        "        \"sample_reference\": multi_references[0] if multi_references else \"\",\n",
        "        \"total_samples\": len(multi_predictions)\n",
        "    }\n",
        "    results.append(multi_result)\n",
        "\n",
        "    # Evaluate on single-turn conversations\n",
        "    print(f\"ðŸ” Evaluating {model_name} on single-turn\")\n",
        "    single_predictions, single_references = evaluate_conversations(\n",
        "        model, tokenizer, single_turn_sample, model_name, \"single\"\n",
        "    )\n",
        "\n",
        "    # Calculate metrics for single-turn\n",
        "    single_metrics = calculate_metrics(single_predictions, single_references)\n",
        "\n",
        "    # Store single-turn results\n",
        "    single_result = {\n",
        "        \"model_name\": model_name,\n",
        "        \"mode\": \"single\",\n",
        "        \"rank\": rank,\n",
        "        \"alpha\": alpha,\n",
        "        \"gradient\": gradient,\n",
        "        \"batch\": batch,\n",
        "        \"learning_rate\": lr,\n",
        "        \"scheduler\": scheduler,\n",
        "        \"dataset\": dataset,\n",
        "        \"epoch\": epoch,\n",
        "        **single_metrics,\n",
        "        \"sample_prediction\": single_predictions[0] if single_predictions else \"\",\n",
        "        \"sample_reference\": single_references[0] if single_references else \"\",\n",
        "        \"total_samples\": len(single_predictions)\n",
        "    }\n",
        "    results.append(single_result)\n",
        "\n",
        "    # Clean up model from memory\n",
        "    print(f\"ðŸ§¹ Cleaning up {model_name} from GPU memory\")\n",
        "    del model, tokenizer\n",
        "    cleanup_gpu_memory()\n",
        "\n",
        "    print(f\"âœ… Completed evaluation for {model_name}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Run evaluation for all models\n",
        "for model_name, model_path in models.items():\n",
        "    evaluate_model_complete(model_name, model_path)\n",
        "\n",
        "# Save results\n",
        "final_df = pd.DataFrame(results)\n",
        "final_df.to_csv(\"model_evaluation_results.csv\", index=False)\n",
        "\n",
        "print(\"ðŸŽ‰ Evaluation completed!\")\n",
        "print(f\"Results saved to model_evaluation_results.csv\")\n",
        "print(f\"Total GPU memory freed: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "print(\"\\nSample results:\")\n",
        "print(final_df[['model_name', 'mode', 'rouge1_fmeasure', 'bleu', 'bertscore_f1', 'meteor', 'embedding_similarity']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24l70Ic5B_5H"
      },
      "outputs": [],
      "source": [
        "final_df = pd.DataFrame(results)\n",
        "final_df.to_csv(\"model_evaluation_results.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqnok1hGWH7m"
      },
      "source": [
        "# **LLM as judge**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_wCj7WuWF8y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import gc\n",
        "import ast\n",
        "import re\n",
        "import time\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# Hyperbolic API configuration\n",
        "HYPERBOLIC_URL = \"https://api.hyperbolic.xyz/v1/chat/completions\"\n",
        "HYPERBOLIC_HEADERS = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJsb2xvMzY3ODc4QGdtYWlsLmNvbSIsImlhdCI6MTc0Mzc1MDMxNX0.MxAWXSR06VjRtgn8mdTQGm7LlG-9Q866s6HIuRZTN1Y\"\n",
        "}\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"test.csv\")\n",
        "\n",
        "def parse_conversation(conv_str):\n",
        "    fixed_str = re.sub(r'\\}\\s*\\{', '}, {', conv_str)\n",
        "    try:\n",
        "        return ast.literal_eval(fixed_str)\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "data[\"parsed_conversations\"] = data[\"conversations\"].apply(parse_conversation)\n",
        "\n",
        "# Filter conversations\n",
        "multi_turn_convs = data[\"parsed_conversations\"].apply(lambda x: len(x) >= 4)\n",
        "single_turn_convs = data[\"parsed_conversations\"].apply(lambda x: len(x) == 2)\n",
        "\n",
        "multi_turn_data = data[multi_turn_convs][\"parsed_conversations\"].tolist()\n",
        "single_turn_data = data[single_turn_convs][\"parsed_conversations\"].tolist()\n",
        "\n",
        "# Sample data (reduced for faster evaluation)\n",
        "multi_turn_sample = random.sample(multi_turn_data, min(100, len(multi_turn_data)))\n",
        "single_turn_sample = random.sample(single_turn_data, min(100, len(single_turn_data)))\n",
        "\n",
        "print(f\"Multi-turn conversations: {len(multi_turn_sample)}\")\n",
        "print(f\"Single-turn conversations: {len(single_turn_sample)}\")\n",
        "\n",
        "# Models configuration\n",
        "models = {\n",
        "    \"llama_base\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"llama_finetuned\": \"kimxxxx/llama_r32_a64_b16_gas2_lr5e-5_4500tk_2epoch\"\n",
        "}\n",
        "\n",
        "def cleanup_gpu_memory():\n",
        "    \"\"\"Force cleanup of GPU memory\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "def call_hyperbolic_judge(prompt, reference_answer, model1_answer, model2_answer, max_retries=3):\n",
        "    \"\"\"Call Hyperbolic API to judge between two model answers\"\"\"\n",
        "\n",
        "    judge_prompt = f\"\"\"You are an expert AI assistant evaluator. Your task is to compare two AI model responses and determine which one is better.\n",
        "\n",
        "**User Prompt:**\n",
        "{prompt}\n",
        "\n",
        "**Reference Answer:**\n",
        "{reference_answer}\n",
        "\n",
        "**Model 1 Answer (llama Base):**\n",
        "{model1_answer}\n",
        "\n",
        "**Model 2 Answer (llama Fine-tuned):**\n",
        "{model2_answer}\n",
        "\n",
        "**Evaluation Criteria:**\n",
        "1. Accuracy and correctness compared to the reference\n",
        "2. Completeness and thoroughness\n",
        "3. Clarity and coherence\n",
        "4. Helpfulness and relevance\n",
        "5. Following instructions properly\n",
        "\n",
        "**Instructions:**\n",
        "- Compare both answers against the reference and evaluation criteria\n",
        "- Choose the better model: \"model1\", \"model2\", or \"tie\" if they are equally good\n",
        "- Provide a clear justification for your choice\n",
        "- Be objective and specific in your reasoning\n",
        "\n",
        "**Response Format:**\n",
        "{{\n",
        "    \"winner\": \"model1|model2|tie\",\n",
        "    \"justification\": \"Your detailed reasoning here\"\n",
        "}}\n",
        "\n",
        "Please respond with valid JSON only.\"\"\"\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            data = {\n",
        "                \"messages\": [{\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": judge_prompt\n",
        "                }],\n",
        "                \"model\": \"meta-llama/Llama-3.3-70B-Instruct\",\n",
        "                \"max_tokens\": 512,\n",
        "                \"temperature\": 0.1,\n",
        "                \"top_p\": 0.9\n",
        "            }\n",
        "\n",
        "            response = requests.post(HYPERBOLIC_URL, headers=HYPERBOLIC_HEADERS, json=data, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            result = response.json()\n",
        "            content = result['choices'][0]['message']['content'].strip()\n",
        "\n",
        "            # Try to parse JSON from the response\n",
        "            try:\n",
        "                # Look for JSON block in the response\n",
        "                json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
        "                if json_match:\n",
        "                    json_content = json_match.group()\n",
        "                    parsed_result = json.loads(json_content)\n",
        "                    return parsed_result\n",
        "                else:\n",
        "                    # Fallback parsing\n",
        "                    if \"model1\" in content.lower() and \"model2\" not in content.lower():\n",
        "                        winner = \"model1\"\n",
        "                    elif \"model2\" in content.lower() and \"model1\" not in content.lower():\n",
        "                        winner = \"model2\"\n",
        "                    elif \"tie\" in content.lower():\n",
        "                        winner = \"tie\"\n",
        "                    else:\n",
        "                        winner = \"tie\"\n",
        "\n",
        "                    return {\n",
        "                        \"winner\": winner,\n",
        "                        \"justification\": content\n",
        "                    }\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"JSON parsing failed for attempt {attempt + 1}, trying again...\")\n",
        "                if attempt == max_retries - 1:\n",
        "                    return {\n",
        "                        \"winner\": \"tie\",\n",
        "                        \"justification\": f\"Failed to parse judge response: {content}\"\n",
        "                    }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling judge API (attempt {attempt + 1}): {e}\")\n",
        "            if attempt == max_retries - 1:\n",
        "                return {\n",
        "                    \"winner\": \"tie\",\n",
        "                    \"justification\": f\"API call failed: {str(e)}\"\n",
        "                }\n",
        "            time.sleep(2)  # Wait before retry\n",
        "\n",
        "    return {\n",
        "        \"winner\": \"tie\",\n",
        "        \"justification\": \"All attempts failed\"\n",
        "    }\n",
        "\n",
        "def generate_response(model, tokenizer, conversation_history):\n",
        "    \"\"\"Generate response from a model given conversation history\"\"\"\n",
        "    try:\n",
        "        # Apply chat template\n",
        "        prompt = tokenizer.apply_chat_template(conversation_history, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Tokenize input\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "            input_length = inputs['input_ids'].shape[1]\n",
        "\n",
        "            # Generate response\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=200,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            # Extract only the generated part\n",
        "            decoded_response = tokenizer.decode(outputs[0, input_length:], skip_special_tokens=True)\n",
        "            prediction = decoded_response.strip()\n",
        "\n",
        "            # Clean up\n",
        "            del inputs, outputs\n",
        "            cleanup_gpu_memory()\n",
        "\n",
        "            return prediction\n",
        "    except Exception as e:\n",
        "        print(f\"Error during generation: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def evaluate_with_judge(conversations, mode_name):\n",
        "    \"\"\"Evaluate models using LLM judge on given conversations\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Load models\n",
        "    print(f\"ðŸ” Loading Phi Base model...\")\n",
        "    model1, tokenizer1 = FastLanguageModel.from_pretrained(\n",
        "        model_name=models[\"llama_base\"],\n",
        "        max_seq_length=5000,\n",
        "        dtype=torch.float16,\n",
        "        load_in_4bit=True\n",
        "    )\n",
        "    tokenizer1 = get_chat_template(tokenizer1, chat_template=\"llama-3.1\")\n",
        "    FastLanguageModel.for_inference(model1)\n",
        "\n",
        "    print(f\"ðŸ” Loading Phi Fine-tuned model...\")\n",
        "    model2, tokenizer2 = FastLanguageModel.from_pretrained(\n",
        "        model_name=models[\"llama_finetuned\"],\n",
        "        max_seq_length=5000,\n",
        "        dtype=torch.float16,\n",
        "        load_in_4bit=True\n",
        "    )\n",
        "    tokenizer2 = get_chat_template(tokenizer2, chat_template=\"llama-3.1\")\n",
        "    FastLanguageModel.for_inference(model2)\n",
        "\n",
        "    total_conversations = len(conversations)\n",
        "    model1_wins = 0\n",
        "    model2_wins = 0\n",
        "    ties = 0\n",
        "\n",
        "    for conv_idx, convo in enumerate(conversations):\n",
        "        print(f\"ðŸ” Processing {mode_name} conversation {conv_idx + 1}/{total_conversations}\")\n",
        "\n",
        "        # Process each user-assistant pair in the conversation\n",
        "        for i in range(0, len(convo), 2):\n",
        "            if i + 1 >= len(convo) or convo[i][\"role\"] != \"user\" or convo[i + 1][\"role\"] != \"assistant\":\n",
        "                continue\n",
        "\n",
        "            # Prepare conversation history up to current user message\n",
        "            history = convo[:i + 1]\n",
        "            reference_answer = convo[i + 1][\"content\"]\n",
        "\n",
        "            # Create prompt string for judge\n",
        "            prompt_str = history[-1][\"content\"]  # Current user message\n",
        "            if len(history) > 1:\n",
        "                # Include previous context\n",
        "                context = \"\"\n",
        "                for msg in history[:-1]:\n",
        "                    context += f\"{msg['role']}: {msg['content']}\\n\"\n",
        "                prompt_str = f\"Context:\\n{context}\\nCurrent question: {prompt_str}\"\n",
        "\n",
        "            # Generate responses from both models\n",
        "            model1_response = generate_response(model1, tokenizer1, history)\n",
        "            model2_response = generate_response(model2, tokenizer2, history)\n",
        "\n",
        "            if not model1_response or not model2_response:\n",
        "                print(\"Skipping due to empty response\")\n",
        "                continue\n",
        "\n",
        "            # Get judge evaluation\n",
        "            print(f\"ðŸ¤– Calling LLM judge...\")\n",
        "            judge_result = call_hyperbolic_judge(\n",
        "                prompt=prompt_str,\n",
        "                reference_answer=reference_answer,\n",
        "                model1_answer=model1_response,\n",
        "                model2_answer=model2_response\n",
        "            )\n",
        "\n",
        "            # Update statistics\n",
        "            if judge_result[\"winner\"] == \"model1\":\n",
        "                model1_wins += 1\n",
        "                best_model = \"llama_base\"\n",
        "            elif judge_result[\"winner\"] == \"model2\":\n",
        "                model2_wins += 1\n",
        "                best_model = \"llama_finetuned\"\n",
        "            else:\n",
        "                ties += 1\n",
        "                best_model = \"tie\"\n",
        "\n",
        "            # Store result\n",
        "            result = {\n",
        "                \"conversation_id\": conv_idx,\n",
        "                \"turn_id\": i // 2,\n",
        "                \"mode\": mode_name,\n",
        "                \"prompt\": prompt_str,\n",
        "                \"reference_answer\": reference_answer,\n",
        "                \"model1_answer\": model1_response,\n",
        "                \"model2_answer\": model2_response,\n",
        "                \"best_model\": best_model,\n",
        "                \"winner\": judge_result[\"winner\"],\n",
        "                \"justification\": judge_result[\"justification\"]\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            print(f\"Winner: {judge_result['winner']}\")\n",
        "\n",
        "            # Save intermediate results\n",
        "            if len(results) % 10 == 0:\n",
        "                temp_df = pd.DataFrame(results)\n",
        "                temp_df.to_csv(f\"temp_judge_results_{mode_name}.csv\", index=False)\n",
        "                print(f\"ðŸ’¾ Saved {len(results)} results temporarily\")\n",
        "\n",
        "    # Clean up models\n",
        "    print(f\"ðŸ§¹ Cleaning up models from GPU memory\")\n",
        "    del model1, tokenizer1, model2, tokenizer2\n",
        "    cleanup_gpu_memory()\n",
        "\n",
        "    # Print statistics\n",
        "    total_comparisons = model1_wins + model2_wins + ties\n",
        "    if total_comparisons > 0:\n",
        "        print(f\"\\nðŸ“Š {mode_name.title()} Results:\")\n",
        "        print(f\"llama Base wins: {model1_wins} ({model1_wins/total_comparisons*100:.1f}%)\")\n",
        "        print(f\"llama Fine-tuned wins: {model2_wins} ({model2_wins/total_comparisons*100:.1f}%)\")\n",
        "        print(f\"Ties: {ties} ({ties/total_comparisons*100:.1f}%)\")\n",
        "        print(f\"Total comparisons: {total_comparisons}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main evaluation function\"\"\"\n",
        "    all_results = []\n",
        "\n",
        "    # Evaluate multi-turn conversations\n",
        "    print(\"ðŸš€ Starting multi-turn evaluation...\")\n",
        "    multi_results = evaluate_with_judge(multi_turn_sample, \"multi_turn\")\n",
        "    all_results.extend(multi_results)\n",
        "\n",
        "    # Evaluate single-turn conversations\n",
        "    print(\"\\nðŸš€ Starting single-turn evaluation...\")\n",
        "    single_results = evaluate_with_judge(single_turn_sample, \"single_turn\")\n",
        "    all_results.extend(single_results)\n",
        "\n",
        "    # Save final results\n",
        "    final_df = pd.DataFrame(all_results)\n",
        "    final_df.to_csv(\"llama_judge_evaluation_results.csv\", index=False)\n",
        "\n",
        "    print(f\"\\nðŸŽ‰ Evaluation completed!\")\n",
        "    print(f\"Results saved to llama_judge_evaluation_results.csv\")\n",
        "    print(f\"Total samples evaluated: {len(all_results)}\")\n",
        "\n",
        "    # Overall statistics\n",
        "    if len(all_results) > 0:\n",
        "        total_base_wins = len(final_df[final_df['winner'] == 'model1'])\n",
        "        total_finetuned_wins = len(final_df[final_df['winner'] == 'model2'])\n",
        "        total_ties = len(final_df[final_df['winner'] == 'tie'])\n",
        "        total_comparisons = len(all_results)\n",
        "\n",
        "        print(f\"\\nðŸ“ˆ Overall Results:\")\n",
        "        print(f\"llama Base wins: {total_base_wins} ({total_base_wins/total_comparisons*100:.1f}%)\")\n",
        "        print(f\"llama Fine-tuned wins: {total_finetuned_wins} ({total_finetuned_wins/total_comparisons*100:.1f}%)\")\n",
        "        print(f\"Ties: {total_ties} ({total_ties/total_comparisons*100:.1f}%)\")\n",
        "\n",
        "        # Show some example results\n",
        "        print(f\"\\nðŸ“‹ Sample Results:\")\n",
        "        sample_results = final_df.head(3)[['prompt', 'best_model', 'justification']]\n",
        "        for idx, row in sample_results.iterrows():\n",
        "            print(f\"\\nExample {idx+1}:\")\n",
        "            print(f\"Prompt: {row['prompt'][:100]}...\")\n",
        "            print(f\"Winner: {row['best_model']}\")\n",
        "            print(f\"Justification: {row['justification'][:200]}...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LOSxaCw8nDT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# DonnÃ©es\n",
        "labels = [\"LLama3.1 Base\", \"Ties\", \"Llama3.1 Fine-tuned\"]\n",
        "values = [29.6, 0.4, 70]  # en pourcentage\n",
        "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]  # bleu, orange, vert\n",
        "\n",
        "# CrÃ©ation du graphique\n",
        "fig, ax = plt.subplots(figsize=(8, 2))\n",
        "\n",
        "# Barre horizontale unique\n",
        "left = 0\n",
        "for val, color, label in zip(values, colors, labels):\n",
        "    ax.barh(0, val, left=left, color=color, edgecolor=\"none\")\n",
        "    ax.text(left + val/2, 0, f\"{label}\\n{val:.1f}%\",\n",
        "            ha=\"center\", va=\"center\", color=\"white\", fontsize=10, fontweight=\"bold\")\n",
        "    left += val\n",
        "\n",
        "# Suppression des axes\n",
        "ax.set_xlim(0, 100)\n",
        "ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HRvr5YSnL5Vd",
        "jtvWzozJMUN-"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}