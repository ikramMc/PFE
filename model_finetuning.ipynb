{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikramMc/PFE/blob/main/model_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExCdRfIqWKt2"
      },
      "source": [
        "# **Installing Required Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5zxz7Lqv4EU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wandb\n",
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_lKgD7gWxXN"
      },
      "source": [
        "# **Model loading and lora configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length =4700\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "model_name = \"unsloth/Llama-3.1-8B-unsloth-bnb-4bit\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name =model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, #Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules =[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 64,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized,don't train biases only weights\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed1OaS6O4k3t"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTzlYjEnY8iE"
      },
      "source": [
        "We use get_chat_template function to get the correct chat template. unsloth support zephyr, chatml, mistral, llama, llama3.1 alpaca, vicuna, vicuna_old ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd1iOOUN6WJi"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "   convos = examples[\"conversations\"]\n",
        "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False).removeprefix('<|begin_of_text|>') for convo in convos]\n",
        "   return { \"text\" : texts, }\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze-uz0Hbv4ET"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "def parse_conversations(conv_str):\n",
        "    try:\n",
        "        return ast.literal_eval(conv_str)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Transformer en liste de dictionnaires\n",
        "dataset_list = [\n",
        "    {\"conversations\": parse_conversations(conv)} for conv in df[\"conversations\"]\n",
        "]\n",
        "hf_dataset = Dataset.from_list(dataset_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ninvxbeIv4EU"
      },
      "outputs": [],
      "source": [
        "# Remove empty conversations\n",
        "hf_dataset = hf_dataset.filter(\n",
        "    lambda example: example[\"conversations\"] and len(example[\"conversations\"]) > 0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-HApNaIfeQG"
      },
      "outputs": [],
      "source": [
        "print(hf_dataset[12][\"conversations\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "We now use `standardize_sharegpt` to convert ShareGPT style datasets into HuggingFace's generic format. This changes the dataset from looking like:\n",
        "```\n",
        "{\"from\": \"system\", \"value\": \"You are an assistant\"}\n",
        "{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n",
        "{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n",
        "```\n",
        "to\n",
        "```\n",
        "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
        "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
        "```\n",
        "in our case our data is already saved in huggingFace's generic format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhSLK4ldv4EU"
      },
      "outputs": [],
      "source": [
        "#to hugging face formart(rule,content)\n",
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "train_dataset = standardize_sharegpt(hf_dataset)\n",
        "train_dataset = train_dataset.map(formatting_prompts_func, batched = True,)#apply the chat template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDH8B_NTv4EU"
      },
      "outputs": [],
      "source": [
        "train_dataset[157][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwY9i_yrv4EU"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"val.csv\")\n",
        "\n",
        "def parse_conversations(conv_str):\n",
        "    try:\n",
        "        return ast.literal_eval(conv_str)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Transformer en liste de dictionnaires\n",
        "dataset_list = [\n",
        "    {\"conversations\": parse_conversations(conv)} for conv in df[\"conversations\"]\n",
        "]\n",
        "hf_dataset_val = Dataset.from_list(dataset_list)\n",
        "#hf_dataset=hf_dataset.select(range(100))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfmvXIzLv4EU"
      },
      "outputs": [],
      "source": [
        "#to hugging face formart(rule,content)\n",
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "test_dataset = standardize_sharegpt( hf_dataset_val)\n",
        "test_dataset = test_dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "# **Train the model**\n",
        "first we setup the experiment tracker wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjqVEXcsv4EV"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import random\n",
        "import wandb\n",
        "\n",
        "# Start a new wandb run to track this script.\n",
        "run = wandb.init(\n",
        "    # Set the wandb entity where your project will be logged (generally your team name).\n",
        "    entity=\"kimx94347-log\",\n",
        "    # Set the wandb project where this run will be logged.\n",
        "    project=\"finetuning_llama2\",\n",
        "    # Track hyperparameters and run metadata.\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer,SFTConfig\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        dataset_text_field=\"text\",  # Adjust if your field name is different\n",
        "        max_seq_length=max_seq_length,\n",
        "        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
        "        dataset_num_proc=2,\n",
        "        #group_by_length=True,\n",
        "        packing=False,  # Can make training 5x faster for short sequences\n",
        "        args=SFTConfig(\n",
        "            per_device_train_batch_size=8,\n",
        "            per_device_eval_batch_size=8,\n",
        "            gradient_accumulation_steps=4,\n",
        "            dataloader_drop_last=True,\n",
        "           # max_grad_norm=2.0,\n",
        "            warmup_steps=100,\n",
        "            num_train_epochs=3,\n",
        "            learning_rate=5e-5,\n",
        "            eval_strategy=\"steps\",\n",
        "            eval_steps=20,\n",
        "            fp16=not is_bfloat16_supported(),\n",
        "            bf16=is_bfloat16_supported(),\n",
        "            logging_steps=1,\n",
        "            optim=\"Adamw_8bit\",\n",
        "            weight_decay=0.01,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            seed=3407,\n",
        "            output_dir=\"outputs_mistral\",  # Path to save model checkpoints\n",
        "            save_strategy=\"epoch\",  # Save model after each epoch\n",
        "            save_total_limit=7,  # Keep only the last 3 checkpoints\n",
        "            report_to=\"wandb\",  # Use this for WandB etc.\n",
        "        ),\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG2lDkLv7Xru"
      },
      "source": [
        "# **extra steps to train on responses only(calculate loss only on the responses part)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS5pdVLP7akk"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
        "    response_part = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hmQM7pPjx5d"
      },
      "source": [
        "test if the function is working correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0nKeU3e7la3"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(trainer.train_dataset[48][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjcu0xU47pyz"
      },
      "outputs": [],
      "source": [
        "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
        "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[48][\"labels\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB2UycmD7v_g"
      },
      "source": [
        "We can see the Instruction prompts are successfully masked!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vexZBOS974to"
      },
      "source": [
        "# **Start the trainer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i24oIOqHv4Eh"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "\n",
        "model_name=\"kimxxxx/llama_r32_64_b8_gas4_lr5e-5_4500tk_3epoch\"\n",
        "model_location=\"outputs_mistral/checkpoint-1164\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_location)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_location)\n",
        "\n",
        "model.push_to_hub(model_name)\n",
        "tokenizer.push_to_hub(model_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}